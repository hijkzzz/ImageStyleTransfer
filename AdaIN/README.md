# AdaIN

## Summary
![here](https://github.com/SerialLain3170/Style-Transfer/blob/master/AdaIN/images/network.png)
- The model with Adaptive Instance Normalization converts content image into stylized image using channel-wise mean and standard deviation of style image. AdaIN enables the network architecture to convert using any style images.
- The equation of AdaIN is as follows.

![here](https://github.com/SerialLain3170/Style-Transfer/blob/master/AdaIN/images/adain.png)  
x indicates content images, on the other hand, y indicates style images.

## Usage
Excute the command line below.
```bash
$ python train.py --alpha <ALPHA>
```
ALPHA is 0<=α<=1. The bigger the ALPHA value is, The more close to the result image is.

## Result
Result generated by my development environment is below.

![here](https://github.com/SerialLain3170/Style-Transfer/blob/master/AdaIN/images/real.png)
![here](https://github.com/SerialLain3170/Style-Transfer/blob/master/AdaIN/images/anime.png)

These images are still close to content image. Details such as hyperparameters are as follows.
- Ths size of image: 256×256
- Batch size: 3
- Using Adam as optimizer
- We use the outputs of Conv1_1, Conv2_1, Conv3_1, Conv4_1 of Vgg16 in the calculation of style loss. The weight of style loss is 10.0.
